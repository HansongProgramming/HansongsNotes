# Defining crawling
- process of collecting data from non-web sources like repositories, database systems and legacy systems
- typically used for large datasets
- crawled data is processed into an organized list of results for later analysis
- Web Crawling focuses on identifying different web pages of a website and extracting data from them
- typically done to collect data from multiple pages and collate the data into one list of organized results
- often makes use of bots to navigate links on a website simulating user movements and interactions
- allows them to connect links within websites forming a web of data of the website's structure
- what search engines use to match search terms with websites
- three types of content { Initial data searched for, starting pages where data is formed, additional targets to crawl}
# Types of crawlers
## in-house web crawlers
- private crawlers developed by orgs
## Commercial Web crawlers
- ready to use web crawlers made by various companies made to be rented or sold
## open-source web crawlers
- develops own crawlers
# Techniques of crawlers
- ### Robustness
	- must be able to avoid traps
- ### Politeness
	- must follow policies of websites to avoid overloading
- ### Site mapping
	- files which contain all links and pages of a website, used to index pages and recrawl site if needed
- ### Reading robots.txt
	- used to manage crawler traffic to your site. 
- ### Data Deduplication
	- eliminate the changes of duplicates in the extracted data, done to remove extra websites or html tags and texts
# Legality of crawlers
- web crawlers, by their nature can be considered very intrusive as they collect data without explicit content
- no laws that prohibit use of web crawlers, but there is for the crawled content
- take consideration of who you get data from
- mind what type you crawl
- consider website policies
# Crawling Steps
- Setup crawler
	- set up all dependencies required
- define data and pages to be crawled
	- specify which page of website to be crawled
- inspect the html elements of the source url/s
	- most important part
	- inspect element
- setup parameters for crawler
	- where you compile your collected data
	- apply robustness and politeness
- test crawler
	- verify if HTML tags and content are successful
	- revisit settings if you fail
# Languages for crawling
- Python "commonly used"
- Java "from XML and JSON"
- PySpider - open source python (tools for link priority)
- Crawler4J - open source java web crawler with multi threaded web crawling features